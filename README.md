# Image Editing with Text Instructions using StyleCLIP

## Overview

This project explores the editing of images generated by StyleGAN using textual instructions through the StyleCLIP method. The main idea is to manipulate images by altering latent vectors in the latent space of StyleGAN to match given textual descriptions.

## Used Models

<!-- - [**StyleGAN3**](https://github.com/NVlabs/stylegan3): A generative model that can create high-quality images with a high degree of control over style and content.
- [**CLIP**](https://github.com/openai/CLIP) (Contrastive Language–Image Pre-Training): A model by OpenAI that links text and visual representations, allowing it to interpret textual descriptions and relate them to images.
- [**ArcFace**](https://github.com/deepinsight/insightface): A model used for face recognition tasks, helping maintain identity during image transformations.

- [**StyleGAN3 encoder**](https://github.com/yuval-alaluf/stylegan3-editing): A model use to image inversion into the latent space and extracting latent vector

- [**VGG16**](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html) A model to extract features out of image. Used for image inversion by vector optimization -->

|Model |Description|
|---|---|
|[**StyleGAN3**](https://github.com/NVlabs/stylegan3)|A generative model that can create high-quality images with a high degree control over style and content|
|[**CLIP**](https://github.com/openai/CLIP)|A model by OpenAI that links text and visual representations, allowing it to interpret textual descriptions and relate them to images|
|[**ArcFace**](https://github.com/deepinsight/insightface)|A model used for face recognition tasks, helping maintain identity during image transformations|
|[**StyleGAN3 encoder**](https://github.com/yuval-alaluf/stylegan3-editing)|A model used to image inversion into the latent space and extracting latent vector|
|[**VGG16**](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html)| A model used to extract features out of image. Used for image inversion by vector optimization|


## Methods

### Optimization of latent vector

The optimization approach in StyleCLIP involves altering the latent vectors of StyleGAN so that the generated images match the textual descriptions. This is achieved by minimizing a loss function that connects images and text descriptions through the CLIP model.

#### Steps

1. **Generate Initial Image**:
    - Select a random latent vector $z$ from the latent space of StyleGAN, generating a style latent vector $w_s \in W+$.
    - Generate an initial image $I = G(w)$ using StyleGAN based on this vector.
2. **Extract Embeddings**:
    - Pass the image $I$ through the CLIP model to obtain an image embedding $E_{image}$.
    - Pass the text description $t$ through the text part of the CLIP model to obtain a text embedding $E_{text}$.
3. **Loss Function**:
    - **CLIP Loss**: Cosine distance between the CLIP embedding of the image and the text embedding.
        $$D_{CLIP}(G(w), t) = 1 - \cos (E_{image}, E_{text})$$
    - **L2 Loss**: Prevents the latent vector from straying too far from the start vector.
        $$\lambda_{L2} \|w - w_s\|^2$$
    - **ID Regularization**: Maintains identity during transformations using a pre-trained ArcFace model.
        $$\mathcal{L}_{ID}(w) = 1 - \langle R(G(w_s)), R(G(w)) \rangle$$
    - **Total Loss**:

        $$\mathcal{L}_{total} = {D}_{CLIP}(G(w), t) + {\lambda}_{L2} \|w - w_s\|^2 + {\lambda}_{ID}\mathcal{L}_{ID}(w)$$

        (In some reason github doesn't like the last formula :D)

#### Example Results

Prompt: **A man with purple hair**

![Purple hair](https://github.com/xomyakes/styleclip/blob/main/blob/optimization_color.jpg?raw=true)

Prompt: **A pretty guy**

<!-- ![Results](optimization_pretty_guy.jpg) -->
![Pretty Guy](https://github.com/xomyakes/styleclip/blob/main/blob/optimization_pretty_guy.jpg?raw=true)


### Latent Mapper

This approach involves training a model to transform latent vectors in the StyleGAN latent space based on textual descriptions provided by the user. One of it primary advantage is that you just need to train it for one description and then you can path any image through this.

#### Steps

1. **Train Latent Mapper**:
    - Latent Mapper is a neural network that transforms the latent vector so that the resulting image matches the textual description.
2. **Architecture**:
    - The architecture splits the StyleGAN layers into three groups: coarse, medium, and fine.
    - For a latent code $w = (w_c, w_m, w_f)$, the mapper is defined as:
        $$M_t(w) = (M_t^c(w_c), M_t^m(w_m), M_t^f(w_f))$$
3. **Loss Function**:
    - **CLIP Loss**:
        $$D_{CLIP}(G(w + M_t(w)), t) = 1 - \cos (E_{image}, E_{text})$$
    - **L2 Loss**:
        $$\lambda_{L2} \|M_t(w)\|^2$$
    - **ID Regularization**:
        $$\mathcal{L}_{ID}(w) = 1 - \langle R(G(w_s)), R(G(w)) \rangle$$

#### Example Results

Prompt: **Glasses**


![Glasses](https://github.com/xomyakes/styleclip/blob/main/blob/mapper_glasses.jpg?raw=true)
<!-- (mapper_glasses.jpg) -->

Prompt: **The witcher**

![Witcher](https://github.com/xomyakes/styleclip/blob/main/blob/mapper_witcher.jpg?raw=true)
<!-- (mapper_witcher.jpg) -->

Prompt: **Joker**

![Joker](https://github.com/xomyakes/styleclip/blob/main/blob/mapper_joker.jpg?raw=true)
<!-- (mapper_joker.jpg) -->

## Requirements

- 1–8 high-end NVIDIA GPUs with at least 12 GB of memory.
- CUDA toolkit 12 or later
- gcc 9 or later
- python 3.9 or later
- ninja-build (for some torch plugins)

## Installation

To install the project dependencies, run:

```bash
git clone https://github.com/xomyakes/styleclip.git
cd styleclip
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
You will also need to install 3-rd party libraries by cloning it inside styleclip folder
Here's their links

- [**StyleGAN3**](https://github.com/NVlabs/stylegan3) - pre-trained stylegan model
- [**StyleGAN3 Editing**](https://github.com/yuval-alaluf/stylegan3-editing) - contains code for pre-trained stylegan encoder (for editing real images)
- [**Arcface**](https://github.com/ronghuaiyang/arcface-pytorch) - model for calculating ID-loss


```bash
git clone https://github.com/NVlabs/stylegan3.git
git clone https://github.com/yuval-alaluf/stylegan3-editing.git
mv stylegan3-editing editing
git clone https://github.com/ronghuaiyang/arcface-pytorch.git
mv arcface-pytorch arcface

```
Next you need to manualy download several pre-trainde model/weights.

1. ReStyle-pSp Human Faces trained on FFHQ dataset (pre-trained stylegan3 encoder weights) from [**here**](https://drive.google.com/file/d/12WZi2a9ORVg-j6d9x4eF-CKpLaURC2W-/view). Or from  [**original project page**](https://github.com/yuval-alaluf/stylegan3-editing/tree/main) 

    Then put downloaded **restyle_pSp_ffhq.pt** file inside the **editing** folder.
2. Arcface model
    
    [**resnet18_110.pth**](https://drive.google.com/drive/folders/14WeZV0o-oMax1p8soUZhezPliqUVp0gD)
    
    Rename it to **arcface.pth** and put inside **arface** folder

3. If you wish pre-trained **latent mappers** from [**here**](https://drive.google.com/drive/folders/18aKO0BrwsBMY_6gcLSe-GN3AzS26AiA_?usp=sharing) and put theirs into **mappers** folder

**P.S.** Renaming downloaded files and put in inside specific folders in all steps above is not necessary and you can just pass paths to src.loader.Loader for load your models but it's highly recommended to easily usage.

## Inference and Training

You can find inference and training of **latent mapper** and **latent vector optimization** in corresponding jupyter notebooks in the root folder:

- **mapper_inference.ipynb**
- **mapper_training.ipynb**
- **optimization.ipynb**

Also inside **mapper_inference.ipynb** you can find example of editing real images


## Server

This repository also provides the ability to host your own server, which can be deployed somewhere and then users can edit images via http-requests.

The server is built on **FastAPI** and separated on two threads which connected by asynchronous queue:

- Main Server Thread

- Processing Images Thread

It needs because of optimization method blocks thread for a ~5 minutes and server broke connections with all users. So we just separate server of processing.

### Endpoints

- **/mappers** 

    method: **GET**

    params: -

    description: Get list of existed latent mappers

- **/edit** 

    method: **POST**

    params:  
        
    - **image**: binary image to process
    - **sid**: OPTIONAL QUERY socketIO sid which connected to the server to notify the client while process of editing is done
    - **prompt**: OPTIONAL QUERY text prompt for image editing  
    - **mapper**: OPTIONAL QUERY chosen pre-trained mapper

    Send image for editing via existed pre-trained latent mappers or latent vector optimization. One of prompt or mapper should be sent. If you send prompt, editing will be via vector optimization else mapper will be used.


-  **/images** 

    method: **GET**

    params: PATH PARAMETER image_path, which you get as response from **/edit** endpoint

To start the server just run the **start_server.sh** script in root_folder. If you want to run server in production it is highly recommended to use **gunicorn** with uvicorn workers instead of **uvicorn** itself.

Also the are some test scripts for server API inside src/tests folder. Scripts should be called from the root folder such as

        python3 ./src/tests/test_get_mappers.py

The frontend was not developed due to the impossibility of testing it.

## Work report

The work report can be found in the report.pdf file in root folder, but it is in Russian.








